# -*- coding: utf-8 -*-
"""Sentiment_Analysis_On_Products.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gt31ZmXq9wP642OYhNczOwx_LE8026Yf

# **`DEPENDED LIBRARIES`**
"""

import pandas as pd                            # Pandas Library for data manipulation and analysis
import numpy as np                             # Numby Library for numerical computing
import matplotlib.pyplot as plt                # Matplotlib Library for visualization
import seaborn as sns                          # Seaborn Library for  visuals
import nltk                                    # Natural Language Toolkit for working with human language data  (Used for Text Preprocessing , tokenization and more )
from nltk.corpus import stopwords              # Stopwords module for removing common words that  they wont provide any useful meaning for analaysis eg: "that" , "and" , "is"
import warnings                                # Working Module allows user to control warning in python code
warnings.filterwarnings('ignore')              # Filter to ingore warnings generated by python code

"""# **`DATA COLLECTION`**

**Download dataset Here  [Click](https://drive.google.com/file/d/1eAgd6zb2Jg3BU-CNoL9mfnglHv29HoyO/view?usp=sharing)**
"""

df = pd.read_csv('https://raw.githubusercontent.com/ashokzdeccode/dataset/main/Reviews.csv')
df.head()

df.columns  # See the features of the dataset

df.info()  # See information about dataset

"""# **`Preprocessing`**

### **Creating New Features For Mobile , Spec , Rating Extraction**
"""

def process_product_data(df):
    names = []       # Mobile model Name
    varients = []    # Spec
    ratings = []     # Rating they got

    for i in df['Product Name']:
        name = i.split('(')[0].strip()
        names.append(str(name))

        try:                                                         # Used exception handling for index error
            var = i.split('(')[1].strip().split(')')[0]              #  OnePlus Nord CE 2 5G (Gray Mirror, 8GB RAM, 128GB Storage) =  ['OnePlus Nord CE 2 5G' , "Gray Mirror, 8GB RAM, 128GB Storage)"] = "Gray Mirror, 8GB RAM, 128GB Storage)" "
            varients.append(var)
        except IndexError:
            varients.append('Not Available')

    for i in df['rating']:
        points = i.split()[0]
        ratings.append(float(points))

    df['Product'] = names
    df['Variant'] = varients
    df['Ratings'] = ratings

    df[['Color', 'Ram', 'Rom']] = df['Variant'].str.split(', |\+ ', expand=True)

    # Update the values in the 'Rom' column
    df['Rom'] = df['Rom'].replace('64 GB Storage', '64GB Storage')

    # Remove space in the 'Ram' column for '4GB RAM'
    df['Ram'] = df['Ram'].replace('4GB RAM ', '4GB RAM')

    return df

# Call the function with your dataframe 'df'
df = process_product_data(df)
df.head()

"""## **DROPPING UN-WANTED COLUMNS**"""

# Before Dropping

df

# Dropping un-wanted columns

df = df.drop(['Unnamed: 0'], axis=1)
df = df.drop(['rating'], axis=1)
df = df.drop(['Product Name'], axis=1)
df = df.drop(['Variant'], axis=1)
df.head()

"""## **VALUE COUNT BASED ON EACH PRODUCTS**"""

df['Product'].value_counts()

#plot the graph for df['Product'].value_counts()

df['Product'].value_counts().plot(kind='bar',figsize=(10,5),xlabel='Product',ylabel='Count',title='Product')
plt.show()

"""## **VALUE COUNTS BASED ON RATINGS**"""

#ploting the rating of the product

df['Ratings'].value_counts().plot(kind='bar',figsize=(10,5),xlabel='Rating',ylabel='Count',title='Rating')
plt.show()

"""## **RATINGS ON EACH PRODUCTS**"""

#groupby the product and plot the rating

df.groupby('Product')['Ratings'].mean().plot(kind='bar',figsize=(10,5),xlabel='Product',ylabel='Rating',title='Rating')
plt.show()

"""## **VALUE COUNTS BASED ON -RAM-**"""

df['Ram'].value_counts()

df['Ram'].value_counts().plot(kind='bar',figsize=(10,5),xlabel='Ram',ylabel='Count',title='Ram')
plt.show()

"""## **VALUE COUNTS BASED ON -ROM-**"""

df['Rom'].value_counts()

df['Rom'].value_counts().plot(kind='bar',figsize=(10,5),xlabel='Rom',ylabel='Count',title='Rom')
plt.show()

df.head()

"""# **`NLP PROCESS`**

## **REVIEW BODY AND TITLE**
"""

df[['Review-Title', 'Review-Body']]

df['Review-Body'][1]  # See Review in data

"""## **SEGMENTIZE THE REVIEW BASED ON -LABEL-**"""

def label_comments(comments):
    labeled_data = []
    for comment in comments:
        if isinstance(comment, str):  # Check if comment is a string
            # Convert the comment to lowercase for case-insensitive matching
            comment_lower = comment.lower()

            # Check for keywords or patterns to assign labels
            if 'battery' in comment_lower:
                label = 'battery'
            elif any(word in comment_lower for word in ['camera', 'video','picture','picture quality']):
                label = 'camera'
            elif 'design' in comment_lower:
                label = 'design'
            elif any(word in comment_lower for word in ['performance', 'hang', 'heat',]):
                label = 'performance'
            elif 'software' in comment_lower:
                label = 'software'
            else:
                label = 'other'
        else:
            label = 'other'  # Assign 'other' label for non-string values

        labeled_data.append((comment, label))
    return labeled_data

# Label the comments automatically
labeled_data = label_comments([str(i) for i in df['Review-Body']])

# Create a DataFrame from the labeled data
df_labeled = pd.DataFrame(labeled_data, columns=['comment', 'label'])

df_labeled.head()

"""## **VALUE COUNTS BASED ON -LABELS-**"""

df_labeled['label'].value_counts()

df_labeled['label'].value_counts().plot(kind='bar',figsize=(10,5),xlabel='Label',ylabel='Count',title='Label')
plt.show()

#get comments where label is other
df_labeled[df_labeled['label'] == 'other']

"""## **STOP WORDS IDENTIFICATION**"""

nltk.download('stopwords')                          #-- download stopwords
stopwords = set(stopwords.words('english'))         #--specify the languages of stopwords

stopwords

"""## **STOP WORDS REMOVAL PROCESS**



```
# Removing Non-Informative Words (Meaningless)
```


"""

# Before Removing the stop words

df['Review-Body'][1]

sw = []
for i in df_labeled['comment']:
    words = i.split()
    words = [word for word in words if word not in stopwords]
    sw.append(' '.join(words))

# After Removed the stop words

sw[1]

"""## **LEMMATIZATION**



```
# Reducing Words To Their Root Form or Dictionary Form
```


"""

# Total Documents Length
len(sw)

sw[0]

#use Spacy lemmatizer to lemmatize the words

import spacy

# Load the English language model in spaCy
nlp = spacy.load('en_core_web_sm')

lem_word = []

for i in sw:
    doc = nlp(i) # Document  -- longer piece of  text
    lem_word.append([token.lemma_ for token in doc])

"""
## **GRAMMATICAL UNDERSTANDING**



```
# TAGGING EACH WORDS WITH ITS PARTS OF SPEECH
```

"""

nltk.download('averaged_perceptron_tagger')

#part of speech tagging

pos_tag = []

for i in lem_word:
    pos = nltk.pos_tag(i)
    pos_tag.append(pos)

pos_tag[1]

"""### **PARTS OF SPEECH TAGGING DOCUMENT**"""

import nltk
nltk.download('tagsets')
print(nltk.help.upenn_tagset())

"""## **JOIN All Words Based On Their POS TAG**"""

import nltk
nltk.download('maxent_ne_chunker')
nltk.download('words')

import nltk
nltk.download('maxent_ne_chunker')

# chucking the frequency of each word in the pos_tag

chunking = []

for i in pos_tag:
  chunk = nltk.chunk.ne_chunk(i)
  chunking.append(chunk)

chunking

"""## **TREE VISUALIZE FOR POS WORDS**"""

chunking[1].pretty_print()       # Here we can see how each is paired with part of speech

chunking[3].pretty_print()

"""## **SENTIMENT ANALYSIS - POLARITY SCORE**

```
# Numerical  score that quantifies the sentiment or emotional tone of a piece of text (Each review about particular product)
```
"""

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer  # Tool
nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()

POS = sid.polarity_scores(input('Enter a sentence: '))
POS

scores = []
for i in chunking:
  word = [str(sent[0]) for sent in i]
  sentence = ' '.join(word)
  polarity_scores= sid.polarity_scores(sentence)   # Calculating polarity scores for every sentences
  scores.append(polarity_scores)

"""### **Compound Score For All Reviews**




"""

df_scores = pd.DataFrame(scores)
df_scores

"""## **HISTOGRAM for the best visualisation of sentiment scores**"""

#plot hist for df_scores

df_scores.hist()

"""## **Which Rating Point Has Got Highest Compound Score**"""

df_labeled

# Adding Features For visuals

df_labeled['Ratings'] = df['Ratings']
df_labeled['compound'] = df_scores['compound']
df_labeled['Product'] = df['Product']

ax = sns.barplot(data=df_labeled, x = 'Ratings', y = 'compound')
ax.set_title('Sentiment Analysis of Reviews')
plt.show()

"""## **MEAN COMPARISION BETWEEN SENTIMENT AND ACTUAL RATINGS ON -LABELS-**"""

mean_sentiment_scores = df_labeled.groupby('label')['compound'].mean()  # Grouped label with value of  compound score

# Calculate mean ratings by category
mean_ratings = df_labeled.groupby('label')['Ratings'].mean()

# Plot bar chart of mean sentiment scores by category
mean_sentiment_scores.plot(kind='bar', xlabel='Category', ylabel='Mean Sentiment Score(compound)', title='Mean Sentiment Score by Category')
plt.show()

mean_ratings.plot(kind='bar', xlabel='Category', ylabel='Mean Ratings(Ratings)', title='Mean Ratings by Category')
plt.show()

# Print mean ratings by category
print(mean_ratings)

"""# **`Result`**"""

df

df_labeled['Product'] = df['Product']

df_labeled['compound'] = df_scores['compound']

df_labeled['Ratings'] = df['Ratings']

"""## **MEAN COMPARISION BETWEEN SENTIMENT AND RATINGS ON -PRODUCTS-**"""

#plot by product

prod_comp = df_labeled.groupby('Product')['compound'].mean()

prod_comp.plot(kind='bar',figsize=(10,5),xlabel='Product',ylabel='Mean Sentiment Score(compound)',title='Mean Sentiment Score by Product')
plt.show()

print(prod_comp.sort_values(ascending=False))


prod_rating = df_labeled.groupby('Product')['Ratings'].mean()

prod_rating.plot(kind='bar',figsize=(10,5),xlabel='Product',ylabel='Mean Ratings(Ratings)',title='Mean Ratings by Product')
plt.show()

print(prod_rating.sort_values(ascending=False))

"""

```
                                                                           Completed
```

"""